{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b295d9b4-366d-46c3-bee8-cc091f0b8572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m704.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.29.2-py3-none-any.whl (468 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m709.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.29.2 safetensors-0.5.3 tokenizers-0.21.0 transformers-4.49.0\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce8f9ff0-035b-4e0b-9d0e-190eacbbff1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.12-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (19 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
      "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.2.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.7.1)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /opt/anaconda3/lib/python3.12/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.8.4-cp312-cp312-macosx_11_0_arm64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m783.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-macosx_11_0_arm64.whl (42 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.12-cp312-cp312-macosx_11_0_arm64.whl (27 kB)\n",
      "Downloading preshed-3.0.9-cp312-cp312-macosx_11_0_arm64.whl (128 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-macosx_11_0_arm64.whl (634 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m634.7/634.7 kB\u001b[0m \u001b[31m326.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading thinc-8.3.4-cp312-cp312-macosx_11_0_arm64.whl (761 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m761.0/761.0 kB\u001b[0m \u001b[31m374.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.2.0-cp312-cp312-macosx_11_0_arm64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m657.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m704.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.1-cp312-cp312-macosx_11_0_arm64.whl (174 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-1.2.0 catalogue-2.0.10 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 murmurhash-1.0.12 preshed-3.0.9 shellingham-1.5.4 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 typer-0.15.2 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4e7089e-3317-4230-b5de-3828e9ff4f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-lg==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m632.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:17\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c42188a9-b4db-42f7-974e-1e353d463c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-macosx_12_0_arm64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/anaconda3/lib/python3.12/site-packages (from lightgbm) (1.26.4)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from lightgbm) (1.13.1)\n",
      "Downloading lightgbm-4.6.0-py3-none-macosx_12_0_arm64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lightgbm\n",
      "Successfully installed lightgbm-4.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0070dd6-cc72-4196-982a-3af9819ad15d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV with comma separator.\n",
      "Loading time: 31.34 seconds\n",
      "Rows with invalid conversation references (treated as standalone): 562050\n",
      "Regularization time: 321.30 seconds\n",
      "NLP feature time: 154.43 seconds\n",
      "Risk Score Distribution:\n",
      "count    1.537843e+06\n",
      "mean     3.785448e-01\n",
      "std      3.677568e-01\n",
      "min      0.000000e+00\n",
      "25%      1.000000e-01\n",
      "50%      4.000000e-01\n",
      "75%      4.000000e-01\n",
      "max      6.200000e+00\n",
      "Name: risk_score, dtype: float64\n",
      "Number of risky tweets (risk_score > 0.5): 310530\n",
      "Sentiment Score Distribution:\n",
      "count    1.537843e+06\n",
      "mean     4.231924e-03\n",
      "std      4.292262e-02\n",
      "min     -1.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      6.666667e-01\n",
      "Name: sentiment_score, dtype: float64\n",
      "Toxicity Level Distribution:\n",
      "count    1.537843e+06\n",
      "mean     1.506110e-02\n",
      "std      5.987312e-02\n",
      "min      0.000000e+00\n",
      "25%      0.000000e+00\n",
      "50%      0.000000e+00\n",
      "75%      0.000000e+00\n",
      "max      1.000000e+00\n",
      "Name: toxicity_level, dtype: float64\n",
      "Emotion Label Counts:\n",
      "emotion_label\n",
      "neutral    1316279\n",
      "joy         132013\n",
      "anger        51676\n",
      "sadness      37875\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample Tweets with Features:\n",
      "Tweet: @Uber_Support don't you guys read the details what we shared in details for any issue.I was supposed to pay total 93.78 nd I h already had uber credit 85.99 after this I only needed to pay 7.79.But driver took 100/- from nd you also charged uber credit. I need uber credit back\n",
      "Sentiment: -0.02, Toxicity: 0.00, Emotion: neutral, Risk: 0.60\n",
      "\n",
      "Tweet: @BofA_Help And the remaining balance was transferred back to me and there was a positive balance left!! And this morning there was 3 charges for\n",
      "Sentiment: 0.00, Toxicity: 0.00, Emotion: neutral, Risk: 0.40\n",
      "\n",
      "Tweet: @115858 your new update sucks dude. It deleted all of my photos and I had pictures of my son when he was a newborn. Not happy\n",
      "Sentiment: 0.00, Toxicity: 0.20, Emotion: anger, Risk: 1.10\n",
      "\n",
      "Tweet: Thank you so much GWR (@GWRHelp) for your understanding and help with my lost tickets. Much appreciated :-)\n",
      "Sentiment: 0.00, Toxicity: 0.00, Emotion: neutral, Risk: 0.00\n",
      "\n",
      "Tweet: Hey @AmericanAir I have a Brazil to US flight booked soon and I can't see anywhere to check I have veg food? Checked the manage my flight page and I can't see any food options - any ideas? How can I check this?\n",
      "Sentiment: 0.00, Toxicity: 0.00, Emotion: neutral, Risk: 0.10\n",
      "\n",
      "Tweet: @ChipotleTweets Done. Please consider. You guys would do nothing but great up here!\n",
      "Sentiment: 0.07, Toxicity: 0.00, Emotion: joy, Risk: 0.40\n",
      "\n",
      "Tweet: Shouldn't I get a discount if my Uber driver has to stop for gas. #uber\n",
      "Sentiment: 0.00, Toxicity: 0.00, Emotion: neutral, Risk: 0.00\n",
      "\n",
      "Tweet: @mediatemplehelp Not a browser issue. It’s your site’s complete support section: https://t.co/0ZOYz9RnTE\n",
      "Sentiment: -0.06, Toxicity: 0.00, Emotion: neutral, Risk: 0.90\n",
      "\n",
      "Tweet: @CenturyLinkHelp I've been waiting for assistance since 8am and I was told I was second on the list.\n",
      "Sentiment: 0.00, Toxicity: 0.00, Emotion: neutral, Risk: 0.40\n",
      "\n",
      "Tweet: @XboxSupport Hello was just Checking back to see if there was a way to figure this out ^CO?\n",
      "Sentiment: 0.00, Toxicity: 0.00, Emotion: neutral, Risk: 0.40\n",
      "\n",
      "Diagnostics time: 0.44 seconds\n",
      "Training LightGBM Model 1...\n",
      "Iteration 0: Train Loss = 0.4667, Test Loss = 0.4656\n",
      "Iteration 10: Train Loss = 0.2964, Test Loss = 0.2959\n",
      "Iteration 20: Train Loss = 0.2220, Test Loss = 0.2217\n",
      "Iteration 30: Train Loss = 0.1862, Test Loss = 0.1860\n",
      "Iteration 40: Train Loss = 0.1654, Test Loss = 0.1653\n",
      "Iteration 50: Train Loss = 0.1519, Test Loss = 0.1519\n",
      "Iteration 60: Train Loss = 0.1436, Test Loss = 0.1436\n",
      "Iteration 70: Train Loss = 0.1376, Test Loss = 0.1376\n",
      "Iteration 80: Train Loss = 0.1332, Test Loss = 0.1334\n",
      "Iteration 90: Train Loss = 0.1300, Test Loss = 0.1302\n",
      "\n",
      "Final Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98    245678\n",
      "           1       0.96      0.88      0.92     61891\n",
      "\n",
      "    accuracy                           0.97    307569\n",
      "   macro avg       0.96      0.94      0.95    307569\n",
      "weighted avg       0.97      0.97      0.97    307569\n",
      "\n",
      "Model saved as model1.pkl, TF-IDF saved as tfidf1.pkl\n",
      "Training time: 29.44 seconds\n",
      "Generating 70 additional columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Generation Progress:  41%|█████▊        | 29/70 [12:44<08:00, 11.71s/it]/var/folders/_3/57plbzqn57d6br7h5pg7rb600000gn/T/ipykernel_26453/424849611.py:285: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df['monthly_sentiment_trend'] = df.groupby(df['created_at'].dt.to_period('M'))['sentiment_score'].transform('mean'); pbar.update(1)\n",
      "Feature Generation Progress:  44%|██████▏       | 31/70 [14:13<13:19, 20.49s/it]/var/folders/_3/57plbzqn57d6br7h5pg7rb600000gn/T/ipykernel_26453/424849611.py:287: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df['company_response_speed_trend'] = df.groupby(df['created_at'].dt.to_period('M'))['customer_response_time'].transform('mean'); pbar.update(1)\n",
      "/var/folders/_3/57plbzqn57d6br7h5pg7rb600000gn/T/ipykernel_26453/424849611.py:288: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df['issue_resolution_time_trend'] = df.groupby(df['created_at'].dt.to_period('M'))['customer_response_time'].transform('mean'); pbar.update(1)\n",
      "Feature Generation Progress:  47%|██████▌       | 33/70 [14:13<09:09, 14.84s/it]/var/folders/_3/57plbzqn57d6br7h5pg7rb600000gn/T/ipykernel_26453/424849611.py:290: UserWarning: Converting to PeriodArray/Index representation will drop timezone information.\n",
      "  df['weekly_customer_sentiment_shift'] = df.groupby([df['author_id'], df['created_at'].dt.to_period('W')])['sentiment_score'].transform(lambda x: x.diff().mean() if len(x) > 1 else 0); pbar.update(1)\n",
      "Feature Generation Progress:  91%|████████████▊ | 64/70 [16:35<01:33, 15.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature generation time: 995.29 seconds\n",
      "Writing enhanced dataset to ready.csv...\n",
      "CSV writing time: 34.05 seconds\n",
      "Total new columns added: 70\n",
      "New columns: ['sentiment_score', 'sentiment_category', 'risk_score', 'toxicity_level', 'customer_angry_flag', 'customer_disappointed_flag', 'customer_praise_flag', 'customer_churn_risk', 'complaint_intensity', 'urgent_issue_flag', 'customer_lifetime_tweet_count', 'customer_interaction_frequency', 'customer_response_time', 'repeat_complainer_flag', 'first_time_complainer_flag', 'customer_loyalty_score', 'customer_lifetime_sentiment_trend', 'customer_issue_recency', 'escalation_probability', 'resolution_time_category', 'issue_type', 'issue_severity', 'refund_request_flag', 'technical_issue_flag', 'service_issue_flag', 'billing_issue_flag', 'product_quality_issue_flag', 'delivery_issue_flag', 'legal_threat_flag', 'social_media_virality_risk', 'time_of_day_category', 'weekend_flag', 'holiday_season_flag', 'monthly_sentiment_trend', 'customer_peak_engagement_time', 'company_response_speed_trend', 'issue_resolution_time_trend', 'yearly_complaint_pattern', 'weekly_customer_sentiment_shift', 'tweet_hour_bucket', 'company_response_time', 'company_response_quality', 'repeat_issue_flag', 'customer_dissatisfaction_trend', 'sentiment_change_after_response', 'crisis_alert_flag', 'support_team_performance_score', 'automation_suitability_score', 'customer_satisfaction_prediction', 'business_impact_score', 'support_related_flag', 'billing_related_flag', 'technical_related_flag', 'operations_related_flag', 'marketing_related_flag', 'compliance_legal_flag', 'app_crash_flag', 'slow_loading_flag', 'login_issue_flag', 'server_downtime_flag', 'feature_request_flag', 'security_concern_flag', 'product_quality_flag', 'warranty_issue_flag', 'pricing_complaint_flag', 'subscription_issue_flag', 'refund_delay_flag', 'delivery_missed_flag', 'mentioned_companies', 'company_mention_count']\n",
      "Total execution time: 1566.89 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# --- Model ID Generator ---\n",
    "def get_next_model_id(base_name='model', extension='.pkl'):\n",
    "    n = 1\n",
    "    while os.path.exists(f\"{base_name}{n}{extension}\"):\n",
    "        n += 1\n",
    "    return n\n",
    "\n",
    "model_id = get_next_model_id()\n",
    "model_filename = f\"model{model_id}.pkl\"\n",
    "tfidf_filename = f\"tfidf{model_id}.pkl\"\n",
    "\n",
    "# --- Robust CSV Loading ---\n",
    "def load_csv_robustly(file_path):\n",
    "    expected_columns = ['tweet_id', 'author_id', 'inbound', 'created_at', 'text', 'response_tweet_id', 'in_response_to_tweet_id']\n",
    "    try:\n",
    "        df = pd.read_csv(file_path, sep=',', engine='python', on_bad_lines='warn', quotechar='\"')\n",
    "        print(\"Loaded CSV with comma separator.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Comma-separated parsing failed: {e}\")\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            lines = f.readlines()\n",
    "        header = lines[0].strip().split(',')\n",
    "        data = [line.strip().split(',', 6) for line in lines[1:]]\n",
    "        df = pd.DataFrame(data, columns=header[:7] if len(header) >= 7 else header + [''] * (7 - len(header)))\n",
    "        print(\"Loaded CSV manually splitting by commas.\")\n",
    "\n",
    "    df.columns = [col.strip() for col in df.columns]\n",
    "    for col in expected_columns:\n",
    "        if col not in df.columns:\n",
    "            df[col] = '' if col in ['text', 'response_tweet_id'] else -1 if col == 'in_response_to_tweet_id' else np.nan\n",
    "    df = df[expected_columns]\n",
    "    return df\n",
    "\n",
    "df = load_csv_robustly('twcs.csv')\n",
    "print(f\"Loading time: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# --- Data Regularization ---\n",
    "def regularize_data(df):\n",
    "    start = time.time()\n",
    "    df['tweet_id'] = pd.to_numeric(df['tweet_id'], errors='coerce')\n",
    "    df['author_id'] = pd.to_numeric(df['author_id'], errors='coerce')\n",
    "    df['in_response_to_tweet_id'] = pd.to_numeric(df['in_response_to_tweet_id'], errors='coerce', downcast='integer')\n",
    "    df['response_tweet_id'] = df['response_tweet_id'].fillna('')\n",
    "    df['in_response_to_tweet_id'] = df['in_response_to_tweet_id'].fillna(-1)\n",
    "\n",
    "    def parse_response_ids(x):\n",
    "        if pd.isna(x) or x == '':\n",
    "            return []\n",
    "        try:\n",
    "            return [int(x.strip())] if str(x).strip().isdigit() else []\n",
    "        except:\n",
    "            return []\n",
    "\n",
    "    df['response_tweet_id'] = df['response_tweet_id'].apply(parse_response_ids)\n",
    "    df['created_at'] = pd.to_datetime(df['created_at'], format='%a %b %d %H:%M:%S %z %Y', errors='coerce', utc=True)\n",
    "    df = df.sort_values('created_at', na_position='first').reset_index(drop=True)\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        text = str(text).lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text)\n",
    "        text = re.sub(r'[\\n\\r\\t]', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s@😡😂😢]', '', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        return text.strip()\n",
    "\n",
    "    df['cleaned_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "    def extract_mentions(text):\n",
    "        words = text.split()\n",
    "        mentions = [word for word in words if word.startswith('@') and len(word) > 1]\n",
    "        return mentions if mentions else []\n",
    "\n",
    "    df['mentioned_companies'] = df['text'].apply(extract_mentions)\n",
    "    df['company_mention_count'] = df['mentioned_companies'].apply(len)\n",
    "\n",
    "    df = df.drop_duplicates(subset=['tweet_id', 'author_id', 'text'], keep='first')\n",
    "    df = df[df['tweet_id'].notna() & df['author_id'].notna()]\n",
    "\n",
    "    def validate_response(row):\n",
    "        if pd.isna(row['in_response_to_tweet_id']) or row['in_response_to_tweet_id'] == -1:\n",
    "            return True\n",
    "        return row['in_response_to_tweet_id'] in df['tweet_id'].values\n",
    "\n",
    "    df['is_conversation_valid'] = df.apply(validate_response, axis=1)\n",
    "    print(f\"Rows with invalid conversation references (treated as standalone): {len(df[~df['is_conversation_valid']])}\")\n",
    "    print(f\"Regularization time: {time.time() - start:.2f} seconds\")\n",
    "    return df\n",
    "\n",
    "df = regularize_data(df)\n",
    "\n",
    "# --- Enhanced NLP Features ---\n",
    "start = time.time()\n",
    "positive_words = ['good', 'great', 'awesome', 'happy', 'love', 'excellent', 'best', 'thanks', 'amazing', 'perfect', 'fixed', 'helpful', 'fast', 'nice', 'appreciate', 'cool', 'sweet', 'yay']\n",
    "negative_words = ['bad', 'terrible', 'awful', 'hate', 'worst', 'poor', 'sucks', 'slow', 'broken', 'disgrace', 'annoying', 'fail', 'horrible', 'issue', 'problem', 'disappointing', 'trash', 'shitty', 'damn', 'wtf']\n",
    "\n",
    "def get_sentiment(text):\n",
    "    words = set(re.findall(r'\\w+', text.lower()))\n",
    "    pos_count = sum(1 for word in words if word in positive_words) + ('😂' in text) * 1\n",
    "    neg_count = sum(1 for word in words if word in negative_words) + ('😡' in text) * 1 + ('😢' in text) * 1\n",
    "    score = (pos_count - neg_count) / max(len(words) + 1, 1)\n",
    "    category = 'Positive' if score > 0.03 else 'Negative' if score < -0.03 else 'Neutral'\n",
    "    return score, category\n",
    "\n",
    "df[['sentiment_score', 'sentiment_category']] = df['text'].apply(lambda x: pd.Series(get_sentiment(x)))\n",
    "\n",
    "toxic_keywords = ['hate', 'stupid', 'idiot', 'worst', 'terrible', 'awful', 'sucks', 'damn', 'hell', 'wtf', 'fuck', 'shit', 'ass', 'pissed', 'bullshit', 'crap', 'trash', 'jerk', 'fucking']\n",
    "\n",
    "def get_toxicity(text):\n",
    "    words = set(re.findall(r'\\w+', text.lower()))\n",
    "    toxic_count = sum(1 for word in words if word in toxic_keywords) + ('😡' in text) * 1\n",
    "    return min(toxic_count / 5.0, 1.0)\n",
    "\n",
    "df['toxicity_level'] = df['text'].apply(get_toxicity)\n",
    "\n",
    "emotion_keywords = {\n",
    "    'anger': ['angry', 'mad', 'furious', 'hate', 'damn', 'wtf', 'fuck', 'annoying', 'pissed', 'outrage', 'frustrated', 'sucks'],\n",
    "    'sadness': ['sad', 'unhappy', 'sorry', 'terrible', 'poor', 'disappointed', 'upset', 'pain', 'hurt'],\n",
    "    'joy': ['happy', 'great', 'awesome', 'love', 'thanks', 'amazing', 'wonderful', 'cool', 'sweet'],\n",
    "    'neutral': []\n",
    "}\n",
    "\n",
    "def get_emotion(text):\n",
    "    words = set(re.findall(r'\\w+', text.lower()))\n",
    "    if '😡' in text:\n",
    "        return 'anger', 0.5\n",
    "    if '😢' in text:\n",
    "        return 'sadness', 0.5\n",
    "    if '😂' in text:\n",
    "        return 'joy', 0.5\n",
    "    for emotion, keywords in emotion_keywords.items():\n",
    "        score = sum(1 for word in words if word in keywords) / max(len(words) + 1, 1)\n",
    "        if score > 0.03:\n",
    "            return emotion, score\n",
    "    return 'neutral', 0.03\n",
    "\n",
    "df[['emotion_label', 'emotion_score']] = df['text'].apply(lambda x: pd.Series(get_emotion(x)))\n",
    "\n",
    "df['risk_score'] = (\n",
    "    df['toxicity_level'] * 1.5 +\n",
    "    (df['sentiment_score'] < 0).astype(int) * 0.5 +\n",
    "    (df['emotion_label'] == 'anger').astype(int) * 0.7 +\n",
    "    (~df['is_conversation_valid']).astype(int) * 0.3 +\n",
    "    df['company_mention_count'] * 0.1 +\n",
    "    df['text'].str.contains('urgent|now|immediately', case=False, na=False).astype(int) * 0.5\n",
    ")\n",
    "\n",
    "print(f\"NLP feature time: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# --- Diagnostics ---\n",
    "start = time.time()\n",
    "print(\"Risk Score Distribution:\")\n",
    "print(df['risk_score'].describe())\n",
    "print(\"Number of risky tweets (risk_score > 0.5):\", len(df[df['risk_score'] > 0.5]))\n",
    "print(\"Sentiment Score Distribution:\")\n",
    "print(df['sentiment_score'].describe())\n",
    "print(\"Toxicity Level Distribution:\")\n",
    "print(df['toxicity_level'].describe())\n",
    "print(\"Emotion Label Counts:\")\n",
    "print(df['emotion_label'].value_counts())\n",
    "print(\"\\nSample Tweets with Features:\")\n",
    "sample = df[['text', 'sentiment_score', 'toxicity_level', 'emotion_label', 'risk_score']].sample(10)\n",
    "for idx, row in sample.iterrows():\n",
    "    print(f\"Tweet: {row['text']}\")\n",
    "    print(f\"Sentiment: {row['sentiment_score']:.2f}, Toxicity: {row['toxicity_level']:.2f}, Emotion: {row['emotion_label']}, Risk: {row['risk_score']:.2f}\\n\")\n",
    "print(f\"Diagnostics time: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# --- Model Training with Updates ---\n",
    "start = time.time()\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X = tfidf.fit_transform(df['cleaned_text'])\n",
    "y = (df['risk_score'] > 0.5).astype(int)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare LightGBM dataset\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Define parameters\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'class_weight': 'balanced',\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Callback for progress updates\n",
    "def log_evaluation(period=10):\n",
    "    def callback(env):\n",
    "        if env.iteration % period == 0:\n",
    "            print(f\"Iteration {env.iteration}: Train Loss = {env.evaluation_result_list[0][2]:.4f}, Test Loss = {env.evaluation_result_list[1][2]:.4f}\")\n",
    "    return callback\n",
    "\n",
    "# Train model\n",
    "print(f\"Training LightGBM Model {model_id}...\")\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=100,\n",
    "    valid_sets=[train_data, test_data],\n",
    "    valid_names=['train', 'test'],\n",
    "    callbacks=[log_evaluation(period=10)]\n",
    ")\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "print(\"\\nFinal Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Save model and vectorizer\n",
    "joblib.dump(model, model_filename)\n",
    "joblib.dump(tfidf, tfidf_filename)\n",
    "print(f\"Model saved as {model_filename}, TF-IDF saved as {tfidf_filename}\")\n",
    "print(f\"Training time: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# --- Optimized Feature Generation with Updates ---\n",
    "start = time.time()\n",
    "print(\"Generating 70 additional columns...\")\n",
    "\n",
    "issue_keywords = {\n",
    "    'billing': ['bill', 'payment', 'charge', 'cost', 'price', 'refund', 'overcharge'],\n",
    "    'technical': ['bug', 'error', 'crash', 'fix', 'tech', 'slow', 'update', 'app', 'down'],\n",
    "    'delivery': ['ship', 'delivery', 'late', 'arrive', 'delay', 'missing'],\n",
    "    'support': ['help', 'support', 'service', 'customer', 'call', 'response'],\n",
    "    'other': []\n",
    "}\n",
    "\n",
    "def classify_issue(text):\n",
    "    words = set(re.findall(r'\\w+', text.lower()))\n",
    "    for issue, keywords in issue_keywords.items():\n",
    "        if any(kw in words for kw in keywords):\n",
    "            return issue\n",
    "    return 'other'\n",
    "\n",
    "# Feature generation with progress\n",
    "feature_steps = 70\n",
    "with tqdm(total=feature_steps, desc=\"Feature Generation Progress\") as pbar:\n",
    "    df['issue_type'] = df['text'].apply(classify_issue); pbar.update(1)\n",
    "    df['customer_angry_flag'] = (df['emotion_label'] == 'anger') & (df['emotion_score'] > 0.7).astype(int); pbar.update(1)\n",
    "    df['customer_disappointed_flag'] = (df['emotion_label'] == 'sadness') & (df['emotion_score'] > 0.7).astype(int); pbar.update(1)\n",
    "    df['customer_praise_flag'] = (df['emotion_label'] == 'joy') & (df['emotion_score'] > 0.7).astype(int); pbar.update(1)\n",
    "    df['customer_churn_risk'] = np.select([df['sentiment_score'] < -0.5, df['sentiment_score'] < 0], ['High', 'Medium'], 'Low'); pbar.update(1)\n",
    "    df['complaint_intensity'] = np.select([df['risk_score'] > 1.0, df['risk_score'] > 0.5], ['severe', 'moderate'], 'mild'); pbar.update(1)\n",
    "    df['urgent_issue_flag'] = df['text'].str.contains('urgent|now|immediately', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['customer_lifetime_tweet_count'] = df.groupby('author_id')['tweet_id'].transform('count'); pbar.update(1)\n",
    "    df['customer_interaction_frequency'] = pd.cut(df['customer_lifetime_tweet_count'], bins=[0, 5, 20, float('inf')], labels=['rarely', 'weekly', 'daily']); pbar.update(1)\n",
    "    df['customer_response_time'] = np.random.randint(1, 48, df.shape[0]); pbar.update(1)\n",
    "    df['repeat_complainer_flag'] = (df.groupby('author_id')['customer_angry_flag'].transform('sum') > 1).astype(int); pbar.update(1)\n",
    "    df['first_time_complainer_flag'] = ((df.groupby('author_id')['tweet_id'].transform('cumcount') == 0) & df['customer_angry_flag']).astype(int); pbar.update(1)\n",
    "    df['customer_loyalty_score'] = np.select([df['customer_praise_flag'] == 1, df['customer_angry_flag'] == 1], ['high', 'low'], 'medium'); pbar.update(1)\n",
    "    df['customer_lifetime_sentiment_trend'] = df.groupby('author_id')['sentiment_score'].transform(lambda x: x.diff().mean() if len(x) > 1 else 0); pbar.update(1)\n",
    "    df['customer_issue_recency'] = df.groupby('author_id')['created_at'].transform(lambda x: (df['created_at'].max() - x.max()).days if x.notna().any() else np.nan); pbar.update(1)\n",
    "    df['escalation_probability'] = df['risk_score'] + df['urgent_issue_flag'] * 0.3; pbar.update(1)\n",
    "    df['resolution_time_category'] = pd.cut(df['customer_response_time'], bins=[0, 12, 24, float('inf')], labels=['fast', 'medium', 'slow']); pbar.update(1)\n",
    "    df['issue_severity'] = np.select([df['risk_score'] > 1.0, df['risk_score'] > 0.5], ['Critical', 'Moderate'], 'Minor'); pbar.update(1)\n",
    "    df['refund_request_flag'] = df['text'].str.contains('refund|money back', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['technical_issue_flag'] = (df['issue_type'] == 'technical').astype(int); pbar.update(1)\n",
    "    df['service_issue_flag'] = ((df['issue_type'] == 'support') & (df['emotion_label'] == 'anger')).astype(int); pbar.update(1)\n",
    "    df['billing_issue_flag'] = ((df['issue_type'] == 'billing') & (df['sentiment_score'] < 0)).astype(int); pbar.update(1)\n",
    "    df['product_quality_issue_flag'] = df['text'].str.contains('quality|broken|defective', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['delivery_issue_flag'] = ((df['issue_type'] == 'delivery') & (df['sentiment_score'] < 0)).astype(int); pbar.update(1)\n",
    "    df['legal_threat_flag'] = df['text'].str.contains('sue|legal|lawyer', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['social_media_virality_risk'] = df['risk_score'] + df['company_mention_count'] * 0.1; pbar.update(1)\n",
    "    df['time_of_day_category'] = pd.cut(df['created_at'].dt.hour, bins=[0, 6, 12, 18, 24], labels=['Night', 'Morning', 'Afternoon', 'Evening'], include_lowest=True); pbar.update(1)\n",
    "    df['weekend_flag'] = (df['created_at'].dt.dayofweek >= 5).astype(int); pbar.update(1)\n",
    "    df['holiday_season_flag'] = df['created_at'].dt.month.isin([11, 12]).astype(int); pbar.update(1)\n",
    "    df['monthly_sentiment_trend'] = df.groupby(df['created_at'].dt.to_period('M'))['sentiment_score'].transform('mean'); pbar.update(1)\n",
    "    df['customer_peak_engagement_time'] = df.groupby('author_id')['created_at'].transform(lambda x: x.dt.hour.mode()[0] if x.notna().any() else np.nan); pbar.update(1)\n",
    "    df['company_response_speed_trend'] = df.groupby(df['created_at'].dt.to_period('M'))['customer_response_time'].transform('mean'); pbar.update(1)\n",
    "    df['issue_resolution_time_trend'] = df.groupby(df['created_at'].dt.to_period('M'))['customer_response_time'].transform('mean'); pbar.update(1)\n",
    "    df['yearly_complaint_pattern'] = df.groupby(df['created_at'].dt.month)['customer_angry_flag'].transform('sum'); pbar.update(1)\n",
    "    df['weekly_customer_sentiment_shift'] = df.groupby([df['author_id'], df['created_at'].dt.to_period('W')])['sentiment_score'].transform(lambda x: x.diff().mean() if len(x) > 1 else 0); pbar.update(1)\n",
    "    df['tweet_hour_bucket'] = pd.cut(df['created_at'].dt.hour, bins=[0, 6, 12, 18, 24], labels=['Early Morning', 'Morning', 'Afternoon', 'Night'], include_lowest=True); pbar.update(1)\n",
    "    df['company_response_time'] = df['customer_response_time']; pbar.update(1)\n",
    "    df['company_response_quality'] = np.where(df['sentiment_score'] > 0, 'positive', 'negative'); pbar.update(1)\n",
    "    df['repeat_issue_flag'] = (df.groupby(['author_id', 'issue_type'])['tweet_id'].transform('count') > 1).astype(int); pbar.update(1)\n",
    "    df['customer_dissatisfaction_trend'] = df.groupby('author_id')['sentiment_score'].transform(lambda x: x.diff().mean() < 0 if len(x) > 1 else False); pbar.update(1)\n",
    "    df['sentiment_change_after_response'] = df.apply(lambda row: 0 if not row['is_conversation_valid'] else row['sentiment_score'], axis=1); pbar.update(1)\n",
    "    df['crisis_alert_flag'] = (df.groupby('issue_type')['risk_score'].transform('mean') > 0.8).astype(int); pbar.update(1)\n",
    "    df['support_team_performance_score'] = df['customer_response_time'].apply(lambda x: 100 if x < 12 else 50 if x < 24 else 25); pbar.update(1)\n",
    "    df['automation_suitability_score'] = np.where(df['issue_type'].isin(['billing', 'technical']), 0.8, 0.4); pbar.update(1)\n",
    "    df['customer_satisfaction_prediction'] = (df['sentiment_score'] > 0).astype(int); pbar.update(1)\n",
    "    df['business_impact_score'] = df['risk_score'] * (df['customer_lifetime_tweet_count'] + 1); pbar.update(1)\n",
    "    df['support_related_flag'] = (df['issue_type'] == 'support').astype(int); pbar.update(1)\n",
    "    df['billing_related_flag'] = (df['issue_type'] == 'billing').astype(int); pbar.update(1)\n",
    "    df['technical_related_flag'] = (df['issue_type'] == 'technical').astype(int); pbar.update(1)\n",
    "    df['operations_related_flag'] = (df['issue_type'] == 'delivery').astype(int); pbar.update(1)\n",
    "    df['marketing_related_flag'] = df['text'].str.contains('promo|ad|marketing', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['compliance_legal_flag'] = df['text'].str.contains('policy|legal|compliance', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['app_crash_flag'] = df['text'].str.contains('crash|freeze', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['slow_loading_flag'] = df['text'].str.contains('slow|lag', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['login_issue_flag'] = df['text'].str.contains('login|sign in', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['server_downtime_flag'] = df['text'].str.contains('down|offline', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['feature_request_flag'] = df['text'].str.contains('add|feature|request', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['security_concern_flag'] = df['text'].str.contains('hack|security|breach', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['product_quality_flag'] = df['text'].str.contains('quality|defect', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['warranty_issue_flag'] = df['text'].str.contains('warranty|guarantee', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['pricing_complaint_flag'] = df['text'].str.contains('price|expensive|cost', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['subscription_issue_flag'] = df['text'].str.contains('subscription|cancel', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['refund_delay_flag'] = df['text'].str.contains('refund|delay', case=False, na=False).astype(int); pbar.update(1)\n",
    "    df['delivery_missed_flag'] = df['text'].str.contains('missed|late', case=False, na=False).astype(int); pbar.update(1)\n",
    "\n",
    "print(f\"Feature generation time: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# --- Chunked CSV Writing ---\n",
    "start = time.time()\n",
    "print(\"Writing enhanced dataset to ready.csv...\")\n",
    "df.to_csv('ready.csv', index=False, chunksize=100000)\n",
    "print(f\"CSV writing time: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# --- List New Columns ---\n",
    "new_columns = [\n",
    "    'sentiment_score', 'sentiment_category', 'risk_score', 'toxicity_level', 'customer_angry_flag',\n",
    "    'customer_disappointed_flag', 'customer_praise_flag', 'customer_churn_risk', 'complaint_intensity',\n",
    "    'urgent_issue_flag', 'customer_lifetime_tweet_count', 'customer_interaction_frequency',\n",
    "    'customer_response_time', 'repeat_complainer_flag', 'first_time_complainer_flag', 'customer_loyalty_score',\n",
    "    'customer_lifetime_sentiment_trend', 'customer_issue_recency', 'escalation_probability',\n",
    "    'resolution_time_category', 'issue_type', 'issue_severity', 'refund_request_flag', 'technical_issue_flag',\n",
    "    'service_issue_flag', 'billing_issue_flag', 'product_quality_issue_flag', 'delivery_issue_flag',\n",
    "    'legal_threat_flag', 'social_media_virality_risk', 'time_of_day_category', 'weekend_flag',\n",
    "    'holiday_season_flag', 'monthly_sentiment_trend', 'customer_peak_engagement_time',\n",
    "    'company_response_speed_trend', 'issue_resolution_time_trend', 'yearly_complaint_pattern',\n",
    "    'weekly_customer_sentiment_shift', 'tweet_hour_bucket', 'company_response_time', 'company_response_quality',\n",
    "    'repeat_issue_flag', 'customer_dissatisfaction_trend', 'sentiment_change_after_response',\n",
    "    'crisis_alert_flag', 'support_team_performance_score', 'automation_suitability_score',\n",
    "    'customer_satisfaction_prediction', 'business_impact_score', 'support_related_flag',\n",
    "    'billing_related_flag', 'technical_related_flag', 'operations_related_flag', 'marketing_related_flag',\n",
    "    'compliance_legal_flag', 'app_crash_flag', 'slow_loading_flag', 'login_issue_flag', 'server_downtime_flag',\n",
    "    'feature_request_flag', 'security_concern_flag', 'product_quality_flag', 'warranty_issue_flag',\n",
    "    'pricing_complaint_flag', 'subscription_issue_flag', 'refund_delay_flag', 'delivery_missed_flag',\n",
    "    'mentioned_companies', 'company_mention_count'\n",
    "]\n",
    "\n",
    "print(f\"Total new columns added: {len(new_columns)}\")\n",
    "print(\"New columns:\", new_columns)\n",
    "print(f\"Total execution time: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76aa2813-f6d5-40dd-a080-4b36a7b65d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
